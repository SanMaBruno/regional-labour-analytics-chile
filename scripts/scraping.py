# ğŸ“Œ Setup inicial y librerÃ­as
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ğŸ“Œ 1. Inicializar driver
def init_driver(headless: bool = True) -> webdriver.Chrome:
    options = Options()
    if headless:
        options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=options
    )
    return driver

# ğŸ“Œ 2. FunciÃ³n para scrapear tabla por ID
def scrape_table(driver, wait, table_id):
    try:
        table = wait.until(EC.presence_of_element_located((By.ID, table_id)))
        html = table.get_attribute('outerHTML')
        df = pd.read_html(html)[0]
        print(f"âœ… Tabla extraÃ­da con ID '{table_id}'.")
        return df
    except Exception as e:
        print(f"âŒ No se encontrÃ³ la tabla con ID '{table_id}': {e}")
        # DEBUG: listar IDs de tablas disponibles
        tables = driver.find_elements(By.TAG_NAME, "table")
        ids = [t.get_attribute("id") for t in tables]
        print(f"ğŸ” IDs de tablas encontrados en la pÃ¡gina: {ids}")
        return pd.DataFrame()

# ğŸ“Œ 3. FunciÃ³n para detectar nÃºmero total de pÃ¡ginas
def get_total_pages(driver, wait):
    try:
        # Buscar elementos de paginaciÃ³n
        pages = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, "PageNumber")))
        total_pages = len(pages)
        print(f"ğŸ” Se detectaron {total_pages} pÃ¡ginas en total")
        return total_pages
    except Exception as e:
        print(f"âš ï¸ No se pudo detectar paginaciÃ³n: {e}")
        return 1  # Asume 1 pÃ¡gina si no hay paginaciÃ³n

# ğŸ“Œ 4. FunciÃ³n para cambiar de pÃ¡gina
def go_to_page(driver, wait, page_number):
    try:
        pages = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, "PageNumber")))
        if page_number < len(pages):
            pages[page_number].click()
            print(f"âœ… Cambiado a la pÃ¡gina {page_number + 1}")
            time.sleep(5)  # espera tras el cambio de pÃ¡gina
            return True
        else:
            print(f"âš ï¸ PÃ¡gina {page_number + 1} no existe")
            return False
    except Exception as e:
        print(f"âŒ Error al cambiar a la pÃ¡gina {page_number + 1}: {e}")
        return False

# ğŸ“Œ 5. FunciÃ³n principal de scraping automÃ¡tico
def scrape_all_pages_automatically(url, table_id, output_prefix="table", headless=True):
    """
    FunciÃ³n principal para hacer scraping automÃ¡tico de todas las pÃ¡ginas disponibles.
    
    Args:
        url (str): URL objetivo para el scraping
        table_id (str): ID de la tabla a extraer
        output_prefix (str): Prefijo para los archivos de salida
        headless (bool): Ejecutar en modo headless o no
    
    Returns:
        pd.DataFrame: DataFrame combinado con todos los datos
    """
    print(f"ğŸš€ Iniciando scraping automÃ¡tico de: {url}")
    print(f"ğŸ¯ Buscando tabla con ID: {table_id}")
    
    # Inicializar driver
    driver = init_driver(headless=headless)
    wait = WebDriverWait(driver, 40)
    
    try:
        # Navegar a la URL
        driver.get(url)
        time.sleep(5)
        
        # Detectar nÃºmero total de pÃ¡ginas
        total_pages = get_total_pages(driver, wait)
        
        # Lista para almacenar todos los DataFrames
        all_dataframes = []
        
        # Iterar por todas las pÃ¡ginas
        for page_num in range(total_pages):
            print(f"\nï¿½ Procesando pÃ¡gina {page_num + 1} de {total_pages}...")
            
            # Si no es la primera pÃ¡gina, navegar a la pÃ¡gina especÃ­fica
            if page_num > 0:
                success = go_to_page(driver, wait, page_num)
                if not success:
                    print(f"âš ï¸ Saltando pÃ¡gina {page_num + 1}")
                    continue
            
            # Extraer tabla de la pÃ¡gina actual
            df = scrape_table(driver, wait, table_id)
            
            if not df.empty:
                # Guardar pÃ¡gina individual
                output_path = f"data/raw/{output_prefix}_page{page_num + 1}.csv"
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
                print(f"âœ… PÃ¡gina {page_num + 1} exportada: {len(df)} filas")
                
                # AÃ±adir a la lista para combinaciÃ³n
                all_dataframes.append(df)
            else:
                print(f"âš ï¸ PÃ¡gina {page_num + 1} estÃ¡ vacÃ­a")
        
        # Combinar todas las pÃ¡ginas
        if all_dataframes:
            df_combined = pd.concat(all_dataframes, ignore_index=True)
            combined_path = f"data/raw/{output_prefix}_combined.csv"
            df_combined.to_csv(combined_path, index=False, encoding='utf-8-sig')
            
            print(f"\nğŸ‰ SCRAPING COMPLETADO:")
            print(f"   ğŸ“Š Total de pÃ¡ginas procesadas: {len(all_dataframes)}")
            print(f"   ğŸ“Š Total de filas extraÃ­das: {len(df_combined)}")
            print(f"   ğŸ“ Archivo combinado: {combined_path}")
            
            return df_combined
        else:
            print("âŒ No se pudieron extraer datos de ninguna pÃ¡gina")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"âŒ Error durante el scraping: {e}")
        return pd.DataFrame()
    
    finally:
        driver.quit()
        print("ğŸ Driver cerrado")

# ğŸ“Œ 6. EjecuciÃ³n principal - CONFIGURACIÃ“N AQUÃ
if __name__ == "__main__":
    # ğŸ”§ CONFIGURACIÃ“N PRINCIPAL - Cambia estos valores segÃºn tu necesidad
    URL_TARGET = "https://stat.ine.cl/Index.aspx?lang=es&SubSessionId=78e0518e-d028-4bf8-8d80-444b7277907c"
    TABLE_ID = "tabletofreeze"
    OUTPUT_PREFIX = "ine_table"  # Los archivos se llamarÃ¡n: ine_table_page1.csv, ine_table_page2.csv, etc.
    HEADLESS_MODE = False  # True para modo silencioso, False para ver el navegador
    
    # Ejecutar scraping automÃ¡tico
    result_df = scrape_all_pages_automatically(
        url=URL_TARGET,
        table_id=TABLE_ID,
        output_prefix=OUTPUT_PREFIX,
        headless=HEADLESS_MODE
    )
    
    # Mostrar resumen final
    if not result_df.empty:
        print(f"\nğŸ“‹ RESUMEN FINAL:")
        print(f"   ğŸ”¢ Columnas: {list(result_df.columns)}")
        print(f"   ğŸ“ Dimensiones: {result_df.shape}")
        print(f"   ğŸ“„ Primeras 3 filas:")
        print(result_df.head(3))
    else:
        print("âŒ No se obtuvieron datos")